{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d810a54",
   "metadata": {},
   "source": [
    "# データダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f1347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8827703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォルダ「data」が存在しない場合は作成する\n",
    "data_dir = \"./data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80225c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダウンロードをする時エラーがありました、これはそのSSLの修正\n",
    "import ssl\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)): \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a3b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd_v2.0.tar.gz&name=JWTDv2.0.tar.gz\"\n",
    "save_path = \"./data/JWTD.tar.gz\"\n",
    "if not os.path.exists(save_path):\n",
    "    urllib.request.urlretrieve(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e9f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルを解凍し、カテゴリー数と内容を確認\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# 解凍\n",
    "tar = tarfile.open(\"./data/JWTD.tar.gz\")\n",
    "tar.extractall(\"./data/JWTD/\")\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2dead73",
   "metadata": {},
   "outputs": [],
   "source": [
    "JWTD_TRAIN = \"./data/JWTD/jwtd_v2.0/train.jsonl\"\n",
    "JWTD_TEST = \"./data/JWTD/jwtd_v2.0/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b2953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "\n",
    "# 行ずつ読んで\n",
    "def load_jwtd_data(path=JWTD_TRAIN):\n",
    "    # ファイルを読む\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        # # 行ずつ読んで\n",
    "        for line in file:\n",
    "            # iteratorを変える\n",
    "            yield json.loads(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a60d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': '239', 'title': 'うすた京介', 'pre_rev': '19775709', 'post_rev': '31820058', 'pre_text': 'セガのマニアであることを公言しており、作中よくセガの商品が登場する（ロボピッチャという昔のおもちゃのピッチングマシーンや、セガサターンで発売された『バーチャコップ』等）。', 'post_text': 'セガのマニアであることを公言しており、作中によくセガの商品が登場する（ロボピッチャという昔のおもちゃのピッチングマシーンや、セガサターンで発売された『バーチャコップ』等）。', 'diffs': [{'pre_str': '', 'post_str': 'に', 'pre_bart_likelihood': -32.5, 'post_bart_likelihood': -21.72, 'category': 'deletion'}], 'lstm_average_likelihood': -3.26}\n",
      "{'page': '239', 'title': 'うすた京介', 'pre_rev': '56097633', 'post_rev': '56097638', 'pre_text': '一時期、の甲本ヒロトと交流があると記されていたが、２０１２年７月２２日、ｔｗｉｔｔｅｒ上での質問に「事実無根」と回答している。', 'post_text': '一時期、甲本ヒロトと交流があると記されていたが、２０１２年７月２２日、ｔｗｉｔｔｅｒ上での質問に「事実無根」と回答している。', 'diffs': [{'pre_str': 'の', 'post_str': '', 'pre_bart_likelihood': -44.55, 'post_bart_likelihood': -21.65, 'category': 'insertion_a'}], 'lstm_average_likelihood': -4.56}\n",
      "{'page': '326', 'title': 'アーミッシュ', 'pre_rev': '19336024', 'post_rev': '27253461', 'pre_text': 'そのため自動車は運転しないが。', 'post_text': 'そのため自動車は運転しない。', 'diffs': [{'pre_str': 'が', 'post_str': '', 'pre_bart_likelihood': -21.22, 'post_bart_likelihood': -10.74, 'category': 'insertion_a'}], 'lstm_average_likelihood': -2.27}\n",
      "{'page': '326', 'title': 'アーミッシュ', 'pre_rev': '65833463', 'post_rev': '65833466', 'pre_text': 'アーミッシュキルトが有名で、着古した服の端切れを集めて作らる。', 'post_text': 'アーミッシュキルトが有名で、着古した服の端切れを集めて作られる。', 'diffs': [{'pre_str': 'る', 'post_str': 'れる', 'pre_bart_likelihood': -26.45, 'post_bart_likelihood': -12.51, 'category': 'deletion'}], 'lstm_average_likelihood': -3.58}\n",
      "{'page': '326', 'title': 'アーミッシュ', 'pre_rev': '62579856', 'post_rev': '64925776', 'pre_text': 'そして、１８歳成人になる（ラムスプリンガを終える）際に、一年の間、アーミッシュのコミュニティから離れた後にーミッシュのコミュニティに戻るか、アーミッシュと絶縁して俗世で暮らすかを選択する事が認められているほとんどのアーミッシュの新成人はそのままアーミッシュであり続けることを選択するといわれる。', 'post_text': 'そして、１８歳成人になる（ラムスプリンガを終える）際に、一年の間、アーミッシュのコミュニティから離れた後にアーミッシュのコミュニティに戻るか、アーミッシュと絶縁して俗世で暮らすかを選択する事が認められているほとんどのアーミッシュの新成人はそのままアーミッシュであり続けることを選択するといわれる。', 'diffs': [{'pre_str': 'にーミッシュ', 'post_str': 'にアーミッシュ', 'pre_bart_likelihood': -74.72, 'post_bart_likelihood': -56.34, 'category': 'deletion'}], 'lstm_average_likelihood': -2.93}\n",
      "{'page': '326', 'title': 'アーミッシュ', 'pre_rev': '20321337', 'post_rev': '27253461', 'pre_text': 'アーミッシュの家庭\\u3000−\\u3000家族のいずれかがアーミッシュから離脱抜した場合は、たとえ親子でも互いの交流が疎遠になる。', 'post_text': 'アーミッシュの家庭\\u3000−\\u3000家族のいずれかがアーミッシュから離脱した場合は、たとえ親子でも互いの交流が疎遠になる。', 'diffs': [{'pre_str': '抜', 'post_str': '', 'pre_typo_correction_sysmtem_likelihood': -13.23, 'post_typo_correction_sysmtem_likelihood': -4.12, 'category': 'others'}], 'lstm_average_likelihood': -3.46}\n",
      "{'page': '531', 'title': '池野恋', 'pre_rev': '4044613', 'post_rev': '4280012', 'pre_text': 'エフエム岩手のキャラクラー「けんたくん」のデザインを担当。', 'post_text': 'エフエム岩手のキャラクター「けんたくん」のデザインを担当。', 'diffs': [{'pre_str': 'キャラクラー', 'post_str': 'キャラクター', 'pre_bart_likelihood': -37.01, 'post_bart_likelihood': -7.79, 'category': 'substitution'}], 'lstm_average_likelihood': -2.68}\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(load_jwtd_data(JWTD_TEST)):\n",
    "    print(line)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf62dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_insertion_data(jwtd_data):\n",
    "    categories = [\"insertion_a\", \"insertion_b\"]\n",
    "    for line in jwtd_data:\n",
    "        if len(line[\"diffs\"]) == 1 and (line[\"diffs\"][0][\"category\"] in categories):\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9389a63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page': '33524',\n",
       "  'title': '大和 (百貨店)',\n",
       "  'pre_rev': '72550844',\n",
       "  'post_rev': '72550863',\n",
       "  'pre_text': '２０１９年２月１５日にのパトリアの運営会社である「七尾都市開発」が破産したことにより、３月に閉店。',\n",
       "  'post_text': '２０１９年２月１５日にパトリアの運営会社である「七尾都市開発」が破産したことにより、３月に閉店。',\n",
       "  'diffs': [{'pre_str': 'の',\n",
       "    'post_str': '',\n",
       "    'pre_bart_likelihood': -31.04,\n",
       "    'post_bart_likelihood': -22.51,\n",
       "    'category': 'insertion_a'}],\n",
       "  'lstm_average_likelihood': -3.25},\n",
       " {'page': '33524',\n",
       "  'title': '大和 (百貨店)',\n",
       "  'pre_rev': '70102421',\n",
       "  'post_rev': '70425997',\n",
       "  'pre_text': 'また２０１６年（平成２８年）には、石川県で初めてのサテライトショップとして野々市市サテライトショップがオープンした。',\n",
       "  'post_text': 'また２０１６年（平成２８年）には、石川県で初めてのサテライトショップとして野々市サテライトショップがオープンした。',\n",
       "  'diffs': [{'pre_str': '市',\n",
       "    'post_str': '',\n",
       "    'pre_bart_likelihood': -27.64,\n",
       "    'post_bart_likelihood': -30.42,\n",
       "    'category': 'insertion_b'}],\n",
       "  'lstm_average_likelihood': -2.94}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(filter_insertion_data(load_jwtd_data(JWTD_TEST)), 101, 103))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d47f21",
   "metadata": {},
   "source": [
    "# Bertモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4cf38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\anaconda3\\envs\\Torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install fugashi unidic-lite transformers datasets\n",
    "from transformers import BertJapaneseTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06743a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_KEEP = '$KEEP'\n",
    "TAG_DELETE = '$DELETE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3ee14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(PRETRAINED_MODEL,\n",
    "                    do_lower_case=False, word_tokenizer_type=\"mecab\",\n",
    "                    subword_tokenizer_type=\"wordpiece\",\n",
    "                    mecab_kwargs={\"mecab_dic\": \"unidic_lite\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67e58920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def create_tags(wrong_text:str=None, correct_text:str=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    誤ってる文章と正解の文章から、いらないトーケンを見つける。\n",
    "    return:\n",
    "        tags: list\n",
    "    \"\"\"\n",
    "    \n",
    "    wrong_tokens = tokenizer.tokenize(wrong_text)\n",
    "    correct_tokens = tokenizer.tokenize(correct_text)\n",
    "\n",
    "    # 必要の誤りだけ\n",
    "    if len(wrong_tokens) != (len(correct_tokens) + 1):\n",
    "        raise ValueError(\"誤り文字は一つ以上\")\n",
    "\n",
    "    tags = []\n",
    "    keep_correct_token = None\n",
    "    for wrong_token in wrong_tokens:\n",
    "        if keep_correct_token is None:\n",
    "            # アンパック（Unpacking）a_token, *remaining = a_doc\n",
    "            correct_token, *correct_tokens = correct_tokens\n",
    "        \n",
    "        # 同時にリストをロープしてるからトーケンが違うならそれは誤り\n",
    "        if wrong_token != correct_token:\n",
    "            tags.append(TAG_DELETE)\n",
    "            keep_correct_token = correct_token\n",
    "        else:\n",
    "            tags.append(TAG_KEEP)\n",
    "            keep_correct_token = None\n",
    "\n",
    "    return tags\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75b305a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['これ', 'は', 'テスト', 'で', 'です'],\n",
       " [2, 11190, 897, 13744, 889, 12461, 3],\n",
       " ['$KEEP', '$KEEP', '$KEEP', '$DELETE', '$KEEP'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"これはテストでです\"), tokenizer(\"これはテストでです\").input_ids, create_tags(\"これはテストでです\", \"これはテストです\",tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8ca9a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['この', '為', '、', '影', '##武', '##者', '説', 'や', '「', '任期', '中', 'に', '別人', 'と', 'の', '入れ替わっ', 'て', 'い', 'た', '」', '等', 'の', '説', 'が', '流れ', 'た', 'こと', 'が', 'ある', '。']\n",
      "['$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$DELETE', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP']\n",
      "この為、影武者説や「任期中に別人との入れ替わっていた」等の説が流れたことがある。\n",
      "この為、影武者説や「任期中に別人と入れ替わっていた」等の説が流れたことがある。\n",
      "\n",
      "['出演', '料', 'は', '自身', 'が', '設立', 'し', 'た', '環境', '保護', '団体', 'と', '、', 'の', '元', 'アメリカ', '副', '大統領', 'アル', '・', 'ゴア', 'の', '地球', '温暖', '化', '防止', '事業', 'に', '寄付', 'さ', 'れ', 'た', 'と', 'いう', '。']\n",
      "['$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$DELETE', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP', '$KEEP']\n",
      "出演料は自身が設立した環境保護団体と、の元アメリカ副大統領アル・ゴアの地球温暖化防止事業に寄付されたという。\n",
      "出演料は自身が設立した環境保護団体と、元アメリカ副大統領アル・ゴアの地球温暖化防止事業に寄付されたという。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in itertools.islice(filter_insertion_data(load_jwtd_data(JWTD_TEST)), 110, 112):\n",
    "    wrong = line[\"pre_text\"]\n",
    "    correct = line[\"post_text\"]\n",
    "    \n",
    "    try:\n",
    "        tags = create_tags(wrong, correct, tokenizer)\n",
    "    \n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    print(tokenizer.tokenize(wrong))\n",
    "    print(tags)\n",
    "    print(wrong)\n",
    "    print(correct)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ea3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jwtd_tags_data(path=JWTD_TRAIN):\n",
    "    for line in filter_insertion_data(load_jwtd_data(path)):\n",
    "        wrong = line[\"pre_text\"]\n",
    "        correct = line[\"post_text\"]\n",
    "        \n",
    "        try:\n",
    "            tags = create_tags(wrong, correct, tokenizer)\n",
    "\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        yield wrong, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b74260",
   "metadata": {},
   "source": [
    "## データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5d3b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "TAG_LABEL_MAP = {\n",
    "    TAG_KEEP: 0,\n",
    "    TAG_DELETE: 1,\n",
    "}\n",
    "LABEL_TAG_MAP = {\n",
    "    0: TAG_KEEP,\n",
    "    1: TAG_DELETE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77a80d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111465,\n",
       " 111465,\n",
       " ['このような言語はし死語と呼ばれ、死語が再び母語として使用される例はほとんどない。',\n",
       "  '待遇表現の面では、文法的・語彙的に発達した敬語体系がありり、叙述される人物同士の微妙な関係を表現する（「待遇表現」の節参照）。',\n",
       "  'という短歌は、冒頭から「ひとひらの」までが「雲」に係る長い修飾語でありる。'],\n",
       " [['$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$DELETE',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP'],\n",
       "  ['$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$DELETE',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP'],\n",
       "  ['$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$DELETE',\n",
       "   '$DELETE',\n",
       "   '$DELETE']])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tags = []\n",
    "list_pre_text = []\n",
    "\n",
    "for text, tags in load_jwtd_tags_data(JWTD_TRAIN):\n",
    "    list_pre_text.append(text)\n",
    "    list_tags.append(tags)\n",
    "    \n",
    "len(list_pre_text), len(list_tags), list_pre_text[0:3], list_tags[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b788aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111465, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>このような言語はし死語と呼ばれ、死語が再び母語として使用される例はほとんどない。</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $DELETE, $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>待遇表現の面では、文法的・語彙的に発達した敬語体系がありり、叙述される人物同士の微妙な関係を...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>という短歌は、冒頭から「ひとひらの」までが「雲」に係る長い修飾語でありる。</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>２０１７年４月月現在、インターネット上の言語使用者数は、英語、中国語、スペイン語、アラビア語...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $DELETE, $KEEP, $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>また、関西で「う」を唇を丸めてで発音する（円唇母音）のに対し、関東では唇を丸めずに発音するの...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pre_text  \\\n",
       "0           このような言語はし死語と呼ばれ、死語が再び母語として使用される例はほとんどない。   \n",
       "1  待遇表現の面では、文法的・語彙的に発達した敬語体系がありり、叙述される人物同士の微妙な関係を...   \n",
       "2              という短歌は、冒頭から「ひとひらの」までが「雲」に係る長い修飾語でありる。   \n",
       "3  ２０１７年４月月現在、インターネット上の言語使用者数は、英語、中国語、スペイン語、アラビア語...   \n",
       "4  また、関西で「う」を唇を丸めてで発音する（円唇母音）のに対し、関東では唇を丸めずに発音するの...   \n",
       "\n",
       "                                                tags  \n",
       "0  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $DELETE, $...  \n",
       "1  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "2  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "3  [$KEEP, $KEEP, $KEEP, $KEEP, $DELETE, $KEEP, $...  \n",
       "4  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'pre_text': list_pre_text, 'tags': list_tags})\n",
    "\n",
    "# 大きさを確認しておく\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "204741dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>詳細は、＃プレイヤー側の用語・設定のデデデの項をを参照。</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>通常、フォースの構造と媒体は、超束積高エネルギー生命体（バイドの切れ端）を用いて生成し製作さ...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ほとんどの魔物が攻撃系の魔法が主体であるのに、強力な盾や回復等、戦闘補助がほとんどというう極...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>「商標登録」がないとする認識のの論拠は何なのでしょう？</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>出場から再出場までの空白年数期間がを長かった歌手</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pre_text  \\\n",
       "0                       詳細は、＃プレイヤー側の用語・設定のデデデの項をを参照。   \n",
       "1  通常、フォースの構造と媒体は、超束積高エネルギー生命体（バイドの切れ端）を用いて生成し製作さ...   \n",
       "2  ほとんどの魔物が攻撃系の魔法が主体であるのに、強力な盾や回復等、戦闘補助がほとんどというう極...   \n",
       "3                        「商標登録」がないとする認識のの論拠は何なのでしょう？   \n",
       "4                           出場から再出場までの空白年数期間がを長かった歌手   \n",
       "\n",
       "                                                tags  \n",
       "0  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "1  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "2  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "3  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "4  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 順番をシャッフルする\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c186ace0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22293,\n",
       " (89172, 2),\n",
       " (22293, 2),\n",
       "                                             pre_text  \\\n",
       " 0                       詳細は、＃プレイヤー側の用語・設定のデデデの項をを参照。   \n",
       " 1  通常、フォースの構造と媒体は、超束積高エネルギー生命体（バイドの切れ端）を用いて生成し製作さ...   \n",
       " 2  ほとんどの魔物が攻撃系の魔法が主体であるのに、強力な盾や回復等、戦闘補助がほとんどというう極...   \n",
       " \n",
       "                                                 tags  \n",
       " 0  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       " 1  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       " 2  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  )"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練と検証データを分割\n",
    "# 全体の2割の文章数\n",
    "len_0_2 = len(df) // 5\n",
    "\n",
    "val_df_tags = df[:len_0_2]\n",
    "train_df_tags = df[len_0_2:]\n",
    "\n",
    "len_0_2, train_df_tags.shape, val_df_tags.shape, val_df_tags[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88047201",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>非戦闘員であるためアンナの手助けをしており、アノー号ではシスターミッチェルとともに生活班をに...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>１９４７年（昭和２２年年）５月１日　−　土浦市神立町に新治郡上大津村立上大津中学校として開校...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>袁紫衣の母親をとの因縁がある。</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $DELETE, $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>五稜郭駅前停留所は、函館市交通局が１９５５年（昭和３０年）１１月２７日の鉄道工場前～五稜郭駅...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>１９９４年　新潟国際情報大学設立（情報文化学部学部）</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pre_text  \\\n",
       "0  非戦闘員であるためアンナの手助けをしており、アノー号ではシスターミッチェルとともに生活班をに...   \n",
       "1  １９４７年（昭和２２年年）５月１日　−　土浦市神立町に新治郡上大津村立上大津中学校として開校...   \n",
       "2                                    袁紫衣の母親をとの因縁がある。   \n",
       "3  五稜郭駅前停留所は、函館市交通局が１９５５年（昭和３０年）１１月２７日の鉄道工場前～五稜郭駅...   \n",
       "4                         １９９４年　新潟国際情報大学設立（情報文化学部学部）   \n",
       "\n",
       "                                                tags  \n",
       "0  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "1  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $DE...  \n",
       "2  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $DELETE, $...  \n",
       "3  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "4  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_tags = train_df_tags.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "train_df_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eff7a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>詳細は、＃プレイヤー側の用語・設定のデデデの項をを参照。</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>通常、フォースの構造と媒体は、超束積高エネルギー生命体（バイドの切れ端）を用いて生成し製作さ...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ほとんどの魔物が攻撃系の魔法が主体であるのに、強力な盾や回復等、戦闘補助がほとんどというう極...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>「商標登録」がないとする認識のの論拠は何なのでしょう？</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>出場から再出場までの空白年数期間がを長かった歌手</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pre_text  \\\n",
       "0                       詳細は、＃プレイヤー側の用語・設定のデデデの項をを参照。   \n",
       "1  通常、フォースの構造と媒体は、超束積高エネルギー生命体（バイドの切れ端）を用いて生成し製作さ...   \n",
       "2  ほとんどの魔物が攻撃系の魔法が主体であるのに、強力な盾や回復等、戦闘補助がほとんどというう極...   \n",
       "3                        「商標登録」がないとする認識のの論拠は何なのでしょう？   \n",
       "4                           出場から再出場までの空白年数期間がを長かった歌手   \n",
       "\n",
       "                                                tags  \n",
       "0  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "1  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "2  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "3  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "4  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc7f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d4602f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トーケン数の長さを見ましょう\n",
    "def map_to_length(x, tokenizer):\n",
    "\n",
    "    x[\"pre_text_len\"] = len(tokenizer(x[\"pre_text\"]).input_ids)\n",
    "    x[\"pre_text_longer_32\"] = int(x[\"pre_text_len\"] > 32)\n",
    "    x[\"pre_text_longer_64\"] = int(x[\"pre_text_len\"] > 64)\n",
    "    x[\"pre_text_longer_128\"] = int(x[\"pre_text_len\"] > 128)\n",
    "    x[\"pre_text_longer_256\"] = int(x[\"pre_text_len\"] > 256)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "407fd916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x0000024D1E069DA0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 22293/22293 [00:05<00:00, 4248.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    do_lower_case=False,\n",
    "    word_tokenizer_type=\"mecab\",\n",
    "    subword_tokenizer_type=\"wordpiece\",\n",
    "    mecab_kwargs={\"mecab_dic\": \"unidic_lite\"}\n",
    ")\n",
    "\n",
    "dataset_from_df = Dataset.from_pandas(val_df_tags)\n",
    "data_stats = dataset_from_df.select(range(len(dataset_from_df))).map(lambda x: map_to_length(x, tokenizer), num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dae67c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pre_text', 'tags', 'pre_text_len', 'pre_text_longer_32', 'pre_text_longer_64', 'pre_text_longer_128', 'pre_text_longer_256'],\n",
       "    num_rows: 22293\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "588dc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_print_stats(x):\n",
    "    sample_size = len(x[\"pre_text_len\"])\n",
    "    print(\n",
    "        \"Pre text Mean: {}, %-Pre text > 32:{}, %-Pre text > 64:{}, %-Pre text > 128:{}, %-Pre text > 256:{}\".format(\n",
    "            sum(x[\"pre_text_len\"]) / sample_size,\n",
    "            sum(x[\"pre_text_longer_32\"]) / sample_size,\n",
    "            sum(x[\"pre_text_longer_64\"]) / sample_size,\n",
    "            sum(x[\"pre_text_longer_128\"]) / sample_size,\n",
    "            sum(x[\"pre_text_longer_256\"]) / sample_size,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41ac0904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22293/22293 [00:00<00:00, 495296.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre text Mean: 43.88776746063787, %-Pre text > 32:0.6384963890010317, %-Pre text > 64:0.1634145247387072, %-Pre text > 128:0.004171713093796259, %-Pre text > 256:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = data_stats.map(\n",
    "    compute_and_print_stats,\n",
    "    batched=True,\n",
    "    batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15b70ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2,\n",
       "  2821,\n",
       "  1848,\n",
       "  896,\n",
       "  1564,\n",
       "  897,\n",
       "  828,\n",
       "  3049,\n",
       "  2000,\n",
       "  1727,\n",
       "  1665,\n",
       "  862,\n",
       "  26452,\n",
       "  1727,\n",
       "  2821,\n",
       "  3],\n",
       " ['[CLS]',\n",
       "  '朝',\n",
       "  '太',\n",
       "  'の',\n",
       "  '名',\n",
       "  'は',\n",
       "  '、',\n",
       "  '橘',\n",
       "  '家',\n",
       "  '圓',\n",
       "  '喬',\n",
       "  'が',\n",
       "  '三遊亭',\n",
       "  '圓',\n",
       "  '朝',\n",
       "  '[SEP]'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"朝太の名は、橘家圓喬が三遊亭圓朝に入門した折にに名乗ったのが最初である。\"\n",
    "\n",
    "encoding = tokenizer(text, max_length=16, padding=\"max_length\",\n",
    "                              truncation=True)\n",
    "\n",
    "encoding.input_ids, tokenizer.convert_ids_to_tokens(encoding.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553b710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Typo_Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_length = 64\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def truncate_labels(self, labels):\n",
    "        # tokenizerと同じように文章の長さから二個を排除する\n",
    "        truncated_labels = labels[:(self.max_length) - 2]\n",
    "        \n",
    "        return truncated_labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pre_text, tags = self.df.loc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(pre_text, max_length=self.max_length, padding=\"max_length\",\n",
    "                                 truncation=True)\n",
    "        \n",
    "        # もし文書は切り捨てるとラベルは切り捨てないなら、長さが異なるからエラーです\n",
    "        labels = [TAG_LABEL_MAP[t] for t in tags]\n",
    "        labels = self.truncate_labels(labels)\n",
    "        \n",
    "        # tokenizerは[CLS]と[SEP]を追加するからラベルも含めて\n",
    "        encoding[\"labels\"] = [0] + labels + [0] + [0] * (\n",
    "            len(encoding.input_ids) - len(labels) - 2)\n",
    "        \n",
    "        # Pytorchテンソルへ, return_tensors=\"pt\"は新しい次元をつけるからめんどくさい\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "\n",
    "        return encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "477153ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89172,\n",
       " 22293,\n",
       " {'input_ids': tensor([    2, 12916,   897,   828,    17, 12730,  1244,   896, 14120,  1025,\n",
       "          11671,   896,   977,   977,   977,   896,  5664,   932,   932, 11854,\n",
       "            829,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Typo_Dataset(train_df_tags, tokenizer)\n",
    "val_dataset = Typo_Dataset(val_df_tags, tokenizer)\n",
    "\n",
    "len(train_dataset), len(val_dataset), val_dataset[0], val_dataset[0][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b76f2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1024\n",
    "# 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc59f6ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 12916,   897,  ...,     0,     0,     0],\n",
      "        [    2, 11699,   828,  ..., 11878, 14467,     3],\n",
      "        [    2, 11834,   896,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2, 11574,   828,  ...,     0,     0,     0],\n",
      "        [    2, 25573,  1026,  ...,     0,     0,     0],\n",
      "        [    2, 11156, 16327,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n",
      "torch.Size([1024, 64])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "print(batch)\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbcdca05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v2\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"$KEEP\",\n",
       "    \"1\": \"$DELETE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"$DELETE\": 1,\n",
       "    \"$KEEP\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32768\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=PRETRAINED_MODEL, num_labels=2)\n",
    "model.config.id2label= LABEL_TAG_MAP\n",
    "model.config.label2id = TAG_LABEL_MAP\n",
    "\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a86894e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練モードに設定\n",
    "model.train()\n",
    "\n",
    "print('ネットワーク設定完了')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c9c57",
   "metadata": {},
   "source": [
    "# 損失について"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03b7af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with  torch.set_grad_enabled(False):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    loss = outputs[\"loss\"]\n",
    "    logits = outputs[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d957a1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['loss', 'logits']),\n",
       " torch.Size([1024, 64, 2]),\n",
       " tensor(0.7426, device='cuda:0'),\n",
       " tensor([[ 0.4149, -0.0088],\n",
       "         [ 0.2802, -0.4258],\n",
       "         [ 0.8826,  0.5264],\n",
       "         [ 0.7072, -0.4151],\n",
       "         [ 0.6948,  0.2499]], device='cuda:0'),\n",
       " tensor([0, 0, 0, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys(), logits.shape, loss, logits[0][0:5], batch[\"labels\"][0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8aa53077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4425, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 損失はロジットと確率の場合は違う\n",
    "torch.nn.CrossEntropyLoss()(logits[0][0:5], batch[\"labels\"][0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18828870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6044, 0.3956],\n",
       "         [0.6695, 0.3305],\n",
       "         [0.5881, 0.4119],\n",
       "         [0.7544, 0.2456],\n",
       "         [0.6094, 0.3906]], device='cuda:0'),\n",
       " tensor(0.5603, device='cuda:0'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][0:5].softmax(dim=1), torch.nn.CrossEntropyLoss()(logits[0][0:5].softmax(dim=1), batch[\"labels\"][0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9f9c4b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1024, 2], got [1024, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(logits, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [1024, 2], got [1024, 64]"
     ]
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(logits, batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09131525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([65536, 2]),\n",
       " torch.Size([65536]),\n",
       " torch.Size([1024, 64, 2]),\n",
       " torch.Size([1024, 64]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.view(-1, 2).shape, batch[\"labels\"].view(-1).shape, logits.shape, batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a5a5ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7426, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(logits.view(-1, 2), batch[\"labels\"].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52e34d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([65536]), torch.Size([65536, 2]), torch.Size([65536]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "active_loss = batch[\"attention_mask\"].view(-1) == 1\n",
    "active_logits = logits.view(-1, 2)\n",
    "active_labels = torch.where(\n",
    "    active_loss, batch[\"labels\"].view(-1), torch.tensor(loss_fct.ignore_index).type_as(batch[\"labels\"])\n",
    ")\n",
    "active_loss.shape, active_logits.shape, active_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e4720b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0'),\n",
       " tensor([   0,    0,    0,  ..., -100, -100, -100], device='cuda:0'),\n",
       " torch.bool,\n",
       " torch.int64,\n",
       " torch.int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_loss, active_labels, active_loss.dtype, active_labels.dtype, batch[\"labels\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "647d1294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7692, device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct(active_logits, active_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f3d4871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7553, device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([0.2, 1]).to(device)\n",
    "loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "loss_fct(active_logits, active_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d130498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 0,  ..., 1, 1, 0],\n",
       "         [1, 1, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 1, 0, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " torch.Size([1024, 64]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, preds = torch.max(outputs[\"logits\"], 2)\n",
    "preds, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "131273fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4761, device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.sum(preds == batch[\"labels\"] , dim=1) / batch[\"labels\"].size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044b639",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0500273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールとClassifierモジュールだけ\n",
    "\n",
    "# 1.まず全部を、勾配計算Falseにしてしまう\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
    "for param in model.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48d867a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Bertの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4},\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "weights = torch.tensor([0.35, 1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14d2bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloaders_dict, criterion, optimizer,\n",
    "                   device, epoch, custom_loss=False):\n",
    "    \n",
    "    # モデルのモードを変わる\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "         \n",
    "        # ミニバッチのサイズ、確率とロスのため\n",
    "        batch_size = dataloaders_dict[phase].batch_size\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        epoch_corrects = 0\n",
    "\n",
    "        pbar = tqdm(dataloaders_dict[phase])\n",
    "\n",
    "        for batch in pbar:\n",
    "            # GPUに移動する\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                if custom_loss:\n",
    "                    # レベルを入れない\n",
    "                    outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"],\n",
    "                                    batch[\"token_type_ids\"])\n",
    "                    \n",
    "                    # ロスの計算\n",
    "                    if batch[\"attention_mask\"] is not None:\n",
    "                        active_loss = batch[\"attention_mask\"].view(-1) == 1\n",
    "                        active_logits = outputs[\"logits\"].view(-1, 2)\n",
    "                        active_labels = torch.where(\n",
    "                            active_loss, batch[\"labels\"].view(-1), \n",
    "                            torch.tensor(criterion.ignore_index).type_as(batch[\"labels\"]))\n",
    "                        \n",
    "                        loss = criterion(active_logits, active_labels)\n",
    "                    \n",
    "                    else:\n",
    "                        # マスクがない場合\n",
    "                        loss = criterion(logits.view(-1, 2), batch[\"labels\"].view(-1))\n",
    "                \n",
    "                # 普通のロス\n",
    "                else:\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs[\"loss\"]\n",
    "                    \n",
    "                _, preds = torch.max(outputs[\"logits\"], 2)\n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # 損失と正解数の合計を更新\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "                # batch acc\n",
    "                epoch_corrects += torch.mean(torch.sum(preds == batch[\"labels\"] , dim=1) / batch[\"labels\"].size(1))\n",
    "\n",
    "                pbar.set_description(f\"loss: {loss}\")\n",
    "                \n",
    "    epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "    # epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "    epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase])\n",
    "\n",
    "    print('Epoch {} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, phase,\n",
    "                                                                epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14d38544",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a4634f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.597691535949707:   2%|▏         | 2/88 [00:18<13:08,  9.17s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 10\u001b[0m     train_one_epoch(model, dataloaders_dict, criterion, optimizer,\n\u001b[0;32m     11\u001b[0m                     device, epoch, custom_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[52], line 61\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloaders_dict, criterion, optimizer, device, epoch, custom_loss)\u001b[0m\n\u001b[0;32m     59\u001b[0m             epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;66;03m# batch acc\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m             epoch_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] , dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     63\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloaders_dict[phase]\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# ネットワークがある程度固定であれば、高速化させる\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, dataloaders_dict, criterion, optimizer,\n",
    "                    device, epoch, custom_loss=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c79c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./token_class_weighted.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d26fc5",
   "metadata": {},
   "source": [
    "# テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9875090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v2\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"$KEEP\",\n",
       "    \"1\": \"$DELETE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"$DELETE\": 1,\n",
       "    \"$KEEP\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32768\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertForTokenClassification\n",
    "from transformers.models.bert_japanese.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    do_lower_case=False,\n",
    "    word_tokenizer_type=\"mecab\",\n",
    "    subword_tokenizer_type=\"wordpiece\",\n",
    "    mecab_kwargs={\"mecab_dic\": \"unidic_lite\"}\n",
    ")\n",
    "TAG_KEEP = '$KEEP'\n",
    "TAG_DELETE = '$DELETE'\n",
    "\n",
    "TAG_LABEL_MAP = {\n",
    "    TAG_KEEP: 0,\n",
    "    TAG_DELETE: 1,\n",
    "}\n",
    "LABEL_TAG_MAP = {\n",
    "    0: TAG_KEEP,\n",
    "    1: TAG_DELETE,\n",
    "}\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(pretrained_model_name_or_path=MODEL_NAME, num_labels=2)\n",
    "model.config.id2label= LABEL_TAG_MAP\n",
    "model.config.label2id = TAG_LABEL_MAP\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9cd5568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32768, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights = torch.load(\"./token_class_weighted.pth\")\n",
    "\n",
    "model.load_state_dict(load_weights)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()   # モデルを検証モードに\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0414740",
   "metadata": {},
   "source": [
    "## テストデータをロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a42162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1118,\n",
       " 1118,\n",
       " ['一時期、の甲本ヒロトと交流があると記されていたが、２０１２年７月２２日、ｔｗｉｔｔｅｒ上での質問に「事実無根」と回答している。',\n",
       "  'そのため自動車は運転しないが。',\n",
       "  '随筆の中で自らの息子に対して１２歳で去勢させる計画を綴っていたり（実際に彼女が息子の性器切断を行ったという話はない）、彼に対するフフェラチオ体験談などにも触れたりしており、ルポライターの谷口玲が纏めた少年への性的虐待についての著書の中で批判を受けている。'],\n",
       " [['$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$DELETE',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP'],\n",
       "  ['$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$DELETE',\n",
       "   '$KEEP'],\n",
       "  ['$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$DELETE',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP',\n",
       "   '$KEEP']])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pre_text = []\n",
    "list_tags = []\n",
    "\n",
    "# change to JWTD_TEST\n",
    "for text, tags in load_jwtd_tags_data(JWTD_TEST):\n",
    "    list_pre_text.append(text)\n",
    "    list_tags.append(tags)\n",
    "    \n",
    "len(list_pre_text), len(list_tags), list_pre_text[0:3], list_tags[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a95c077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1118, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>２００８年４月２６日、第１回福福島競馬５日目第６レースのサラブレット３歳未勝利戦にてデビュー。</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航空作戦国家センター（ＣＮＯＡ）が置かれている他、第０５．９４２探知管制センターは南部フラン...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>しかし直親も、「遠州錯乱」で小野政直の息子・小野道好の讒言により、主君の今川氏真から松平元康...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>イメージは６ｒｄ方式とは逆に、途中のＩＰｖ６空間にＩＰｖ４の信号を流すためのトンネルを設定し...</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>プルートはドアの鍵穴を覗くと、そこには誘拐されたろロニーがいた。</td>\n",
       "      <td>[$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pre_text  \\\n",
       "0    ２００８年４月２６日、第１回福福島競馬５日目第６レースのサラブレット３歳未勝利戦にてデビュー。   \n",
       "1  航空作戦国家センター（ＣＮＯＡ）が置かれている他、第０５．９４２探知管制センターは南部フラン...   \n",
       "2  しかし直親も、「遠州錯乱」で小野政直の息子・小野道好の讒言により、主君の今川氏真から松平元康...   \n",
       "3  イメージは６ｒｄ方式とは逆に、途中のＩＰｖ６空間にＩＰｖ４の信号を流すためのトンネルを設定し...   \n",
       "4                   プルートはドアの鍵穴を覗くと、そこには誘拐されたろロニーがいた。   \n",
       "\n",
       "                                                tags  \n",
       "0  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "1  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "2  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "3  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  \n",
       "4  [$KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KEEP, $KE...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'pre_text': list_pre_text, 'tags': list_tags})\n",
    "\n",
    "# 大きさを確認してお\n",
    "print(df.shape)\n",
    "\n",
    "# 順番をシャッフルする\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a40b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1118,\n",
       " {'input_ids': tensor([    2, 11431,  2181,    34,  2812, 11436,  2719,   828,  4036,    31,\n",
       "           1708,  3933, 12745, 12385,    35,  2719,  3803,  4036,    36, 11995,\n",
       "            896, 13423, 14731, 11160,    33,  3099,  2826, 11763,  2470, 11323,\n",
       "          11617,   829,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])},\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Typo_Dataset(df, tokenizer)\n",
    "\n",
    "len(test_dataset), test_dataset[0], test_dataset[0][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6078d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e4b70c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 11431,  2181,  ...,     0,     0,     0],\n",
      "        [    2, 11601, 12051,  ...,     0,     0,     0],\n",
      "        [    2, 11258,  3805,  ..., 11158,   867,     3],\n",
      "        ...,\n",
      "        [    2, 31391,  2713,  ...,  1015, 19907,     3],\n",
      "        [    2, 11145,  2293,  ...,     0,     0,     0],\n",
      "        [    2, 12809,  1025,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n",
      "torch.Size([128, 64])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "print(batch)\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2245f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24980299174785614: 100%|██████████| 9/9 [00:07<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2239 Acc: 0.9137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 損失関数の設定\n",
    "weights = torch.tensor([0.35, 1]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ミニバッチのサイズ\n",
    "batch_size = test_dataloader.batch_size\n",
    "\n",
    "epoch_loss = 0.0\n",
    "epoch_corrects = 0\n",
    "\n",
    "pbar = tqdm(test_dataloader)\n",
    "custom_loss = True\n",
    "model.eval()\n",
    "\n",
    "for batch in pbar:\n",
    "    # GPUに移動する\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    #   optimizer.zero_grad()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        if custom_loss:\n",
    "            # to define a custom loss for the weights\n",
    "            # input_ids=batch[\"input_ids\"]\n",
    "            # attention_mask=batch[\"attention_mask\"]\n",
    "            # token_type_ids=batch[\"token_type_ids\"]\n",
    "\n",
    "            # labels = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"])\n",
    "\n",
    "            # print(outputs.keys(), outputs[\"logits\"].shape, batch[\"labels\"].shape)\n",
    "            #dict_keys(['logits']) torch.Size([128, 128, 2]) torch.Size([128, 128])\n",
    "            if batch[\"attention_mask\"] is not None:\n",
    "                active_loss = batch[\"attention_mask\"].view(-1) == 1\n",
    "                active_logits = outputs[\"logits\"].view(-1, 2)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, batch[\"labels\"].view(-1), torch.tensor(criterion.ignore_index).type_as(batch[\"labels\"])\n",
    "                )\n",
    "                # print(active_logits.shape, active_labels.shape)\n",
    "                loss = criterion(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = criterion(logits.view(-1, 2), batch[\"labels\"].view(-1))\n",
    "\n",
    "            # loss = criterion(outputs[\"logits\"], batch[\"labels\"])\n",
    "\n",
    "        else:\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "        _, preds = torch.max(outputs[\"logits\"], 2)\n",
    "\n",
    "\n",
    "        # 損失と正解数の合計を更新\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        # epoch_corrects += torch.sum(preds == batch[\"labels\"])\n",
    "        # batch acc\n",
    "        epoch_corrects += torch.mean(torch.sum(preds == batch[\"labels\"] , dim=1) / batch[\"labels\"].size(1))\n",
    "\n",
    "        pbar.set_description(f\"loss: {loss}\")\n",
    "\n",
    "epoch_loss = epoch_loss / len(test_dataloader.dataset)\n",
    "# epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "epoch_acc = epoch_corrects.double() / len(test_dataloader)\n",
    "\n",
    "print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1e00b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_remove(model, sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 今回はreturn_tensors=\"pt\"を使って、バッチのように扱うことが出来ます\n",
    "        encoding = tokenizer(sentence, max_length=64, padding=\"max_length\",\n",
    "                            truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "        \n",
    "        outputs = model(**encoding)\n",
    "        logits = outputs[\"logits\"]\n",
    "        preds = logits[0].argmax(-1).detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # print(preds[1:-1])は[CLS]と[SEP]のトーケンを排除する\n",
    "        return preds[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70d29431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これは \u001b[31mは\u001b[0m あん##まり面白##くないテスト\n",
      "直##ぐにエラー \u001b[31mが\u001b[0m  \u001b[31mが\u001b[0m 見つけるだろう\n",
      "今日はちょっと雲雲が多い。\n",
      "今日はちょっと雲光が多い。\n",
      "最近のパソコンは結##構性能 \u001b[31m能\u001b[0m がいいじゃない?\n",
      "ああん##まりいいことを書##けないけど\n",
      "これは問題ないと思う。\n"
     ]
    }
   ],
   "source": [
    "for text in [\n",
    "    \"これははあんまり面白くないテスト\",\n",
    "    \"直ぐにエラーがが見つけるだろう\",\n",
    "    \"今日はちょっと雲雲が多い。\",\n",
    "    \"今日はちょっと雲光が多い。\",\n",
    "    \"最近のパソコンは結構性能能がいいじゃない？\",\n",
    "    \"ああんまりいいことを書けないけど\",\n",
    "    \"これは問題ないと思う。\"\n",
    "      ]:\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    predictions = prediction_remove(model, text)\n",
    "    \n",
    "    for token, pred in zip(tokenized_text, predictions):\n",
    "        if pred == 1:\n",
    "            # 赤いいるで表示\n",
    "            print(f\" \\033[31m{token}\\033[0m \", end=\"\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"{token}\", end=\"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
