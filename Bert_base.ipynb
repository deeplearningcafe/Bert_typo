{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ceddd0e-9268-444d-b468-b557c55a9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1082932-9b34-400c-b970-81e0a2098cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最初はconfig辞書の作成\n",
    "\n",
    "config = {\n",
    "  \"architectures\": [\n",
    "    \"BertForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 32000\n",
    "}\n",
    "config['hidden_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b453daa-8e86-4469-9f5e-150e371024cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Bertは三つの埋め込みがある、単語のため、トーケンタイプのため、位置情報のため\n",
    "    単語IDに分散表現に入れて、その後トーケンタイプを足し算して、最後に位置情報も足し算します\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token Embedding: 単語IDをベクトル化、ボキャブラリーサイズは32000と隠れ次元は768\n",
    "        self.word_embeddings = nn.Embedding(config[\"vocab_size\"], config[\"hidden_size\"],\n",
    "                                           padding_idx=config[\"pad_token_id\"])\n",
    "\n",
    "        # Token Type Embedding:\n",
    "        self.token_type_embeddings = nn.Embedding(config[\"type_vocab_size\"], config[\"hidden_size\"])\n",
    "        \n",
    "        # Position Embedding: 位置情報ベクトルを作る、このmax_position_embeddingsは文章の最大の長さ。\n",
    "        self.position_embeddings = nn.Embedding(config[\"max_position_embeddings\"], config[\"hidden_size\"])\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config[\"hidden_size\"], eps=config[\"layer_norm_eps\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None, inputs_embeds=None):\n",
    "        \"\"\"\n",
    "        [batch_size, seq_length]\n",
    "        input_ids: 単語IDの羅列です。\n",
    "        token_type_ids: 一つ目の文章の場合なら、0、一つ目の文章の場合なら、1。\n",
    "        position_ids: 位置情報。\n",
    "        input_embeds: もし既に単語は分散表現の形で。[batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        if input_ids is not None:\n",
    "            # [batch_size, seq_length]\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = input_embeds.size()[:-1]\n",
    "    \n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        # 1. Token Embedding\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "    \n",
    "        # 2. Token Type Embedding\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=input_ids.device)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "    \n",
    "        # 3. Position Embedding\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(\n",
    "                    seq_length, dtype=torch.long, device=input_ids.device)\n",
    "            # batch_sizeを入れて\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "    \n",
    "        # 全部の埋め込みを足し算して[batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "    \n",
    "        # LayerNormとDropout\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf20d2f3-41da-457b-923d-e474c5fc7d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8970,  1.8419, -0.4941,  ..., -0.8248,  3.2126,  0.2309],\n",
       "          [-0.9316,  0.0000, -0.7703,  ..., -1.2291,  3.1939,  1.0356],\n",
       "          [-1.1881,  0.9460, -0.1656,  ..., -1.3448,  2.9268,  0.3051],\n",
       "          ...,\n",
       "          [-0.4960,  2.3689, -0.0327,  ..., -1.1537,  1.9008, -1.9447],\n",
       "          [ 0.0000,  1.5613,  0.0346,  ...,  0.5873,  1.7842,  0.0000],\n",
       "          [-0.0523,  0.1368,  0.1890,  ..., -0.0105,  2.4563, -0.0000]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " torch.Size([1, 256, 768]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(3, 5, (1,256), dtype=torch.long)\n",
    "embeddings = BertEmbeddings(config)\n",
    "\n",
    "embeddings(a), embeddings(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b85f148-bc47-4d12-b720-bc9e6fc71ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Self Attention\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        self.attention_head_size = int(config[\"hidden_size\"]/config[\"num_attention_heads\"])\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Attentionの全結合層\n",
    "        self.query = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.key = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.value = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        \"\"\"\n",
    "        Multi Head Attentionのためにテンソルの形を変わる。\n",
    "            num_attention_heads=12\n",
    "            hidden_size=768\n",
    "        [batch_size, seq_len, hidden] -> [batch_size, num_attention_heads, seq_len, hidden/num_attention_heads]\n",
    "        最初のHeadは隠れ次元０から63、二つ目のHeadは64から127、．．．\n",
    "        \"\"\"\n",
    "        # x.size()[:-1]これは隠れ次元を除くしてる\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        \n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attention=False):\n",
    "        \"\"\"\n",
    "        hidden_states: Embeddingsモジュールもしくは前段のBertLayerからの出力。[batch_size, seq_len, hidden]\n",
    "        attention_mask: Paddingsはソフトマックスに影響を与えないようにする。[batch_size, 1, 1, seq_len]\n",
    "        output_attention: Attentionの重みを出力するか\n",
    "        \"\"\"\n",
    "\n",
    "        # 入力を全結合層で特徴量変換（注意、multi-head Attentionの全部をまとめて変換しています）\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # Headsを付ける[batch_size, seq_len, 768] -> [batch_size, 12, seq_len, 64]\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Attentionを計算する、クエリーとキーを掛け算する、類似度を得る\n",
    "        # [batch_size, 12, seq_len, 64] x [batch_size, 12, 64, seq_len] -> [batch_size, 12, seq_len, seq_len]\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # マスクをかけます\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "            # このマスクはBertModelのforwardで作成されます、この後ソフトマックスをかけるから\n",
    "            # 無視したい値があるでしょう。そのために掛け算じゃない、足し算のはソフトマックスの\n",
    "            # 仕組みのためです。SoftMax(0)は高い値かもしれない、けどSoftMax(-inf)なら必ず0になる。\n",
    "            # attention_maskの形は[batch_size, 1, 1, seq_len]\n",
    "            # それは全部の[num_heads, seq_len, seq_len]に足し算するpaddingの部分は興味ない。\n",
    "            #だから全部のattention_scoresにマスクを使う\n",
    "        \n",
    "            #extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "            # https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L964\n",
    "\n",
    "        # Attentionを正規化する、確率を得る\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Attention Mapを掛け算します\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # 前の形に戻る[batch_size, 12, seq_len, 64]-> [batch_size, seq_len, 12, 64]\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        # [batch_size, seq_len, 12, 64] -> [batch_size, seq_len, 768]\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attention else (context_layer,)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33131cf1-83c8-48b8-a532-6d3748b4ae6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 128, 128]), torch.Size([1, 12, 64, 128]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn(1, 12, 128, 64)\n",
    "key = torch.randn(1, 12, 128, 64)\n",
    "\n",
    "torch.matmul(query, key.transpose(-1, -2)).shape, key.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a94cb-0021-42c6-9b3d-d671a7878120",
   "metadata": {},
   "source": [
    "MxN NxM -> MxM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ddce90-a711-4e4b-bc30-89bb2a66181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfoutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.LayerNorm = nn.LayerNorm(config[\"hidden_size\"], eps=config[\"layer_norm_eps\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # この全結合層はMulti-HeadAttentionの部分です\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        # Add and Normの部分\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f927d07b-9cd1-4aa1-b863-3fae7a9c7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertSelfAttentionとBertSelfOutputを使う\n",
    "class BertAttention(nn.Module):\n",
    "    '''BertLayerモジュールのSelf-AttentionとAdd and Normの部分です'''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfoutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attention=False):\n",
    "        \"\"\"\n",
    "        hidden_states: Embeddingsモジュールもしくは前段のBertLayerからの出力。[batch_size, seq_len, hidden]\n",
    "        attention_mask: Paddingsはソフトマックスに影響を与えないようにする。[batch_size, 1, 1, seq_len]\n",
    "        output_attention: Attentionの重みを出力するか\n",
    "        \"\"\"\n",
    "        self_outputs = self.self(hidden_states, attention_mask, output_attention)\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "\n",
    "        # もしAttentionがある場合に追加する\n",
    "        outputs = (attention_output,) + self_outputs[1:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e27ecc87-d4fa-4ad5-a1e2-63b96d6dcedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[-0.2125, -1.0476, -0.6899,  ...,  0.6433,  1.2850, -2.1511],\n",
       "           [-0.2007, -0.4193,  1.3287,  ..., -1.2475, -1.0911,  0.3013],\n",
       "           [-0.4148, -0.6579, -0.0326,  ...,  0.0058,  2.0633, -0.1529],\n",
       "           ...,\n",
       "           [ 0.2762, -0.1312,  2.6012,  ..., -0.7131, -0.2553,  0.5749],\n",
       "           [ 0.0839,  0.0355,  1.4692,  ...,  0.1210, -0.2201, -0.1695],\n",
       "           [ 0.3934,  0.1386,  1.7520,  ...,  0.1448,  0.5831, -0.6635]]],\n",
       "         grad_fn=<NativeLayerNormBackward0>),),\n",
       " 1,\n",
       " torch.Size([1, 256, 768]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(3, 5, (1,256), dtype=torch.long)\n",
    "embeddings = BertEmbeddings(config)\n",
    "\n",
    "input_tensor = embeddings(a)\n",
    "attention = BertAttention(config)\n",
    "\n",
    "attention(input_tensor), len(attention(input_tensor)), attention(input_tensor)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225008ca-ce40-4b18-8ad4-a09a30d7df79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([1, 256, 768]), torch.Size([1, 12, 256, 256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = attention(input_tensor, output_attention=True)\n",
    "len(outputs), outputs[0].shape, outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496f94d1-8b17-4384-8a81-d8a79741f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPの部分\n",
    "class BertIntermediate(nn.Module):\n",
    "    '''BertのTransformerBlockモジュールのFeedforwardです'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2aab55b-b84d-4345-ac44-02104be4ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    \"\"\"Add and Normの部分\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.LayerNorm = nn.LayerNorm(config[\"hidden_size\"], eps=config[\"layer_norm_eps\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"\n",
    "        hidden_states: BertIntermediateモジュールからの出力。[batch_size, seq_len, intermediate_size]\n",
    "        input_tensor: Attentionモジュールからの出力。[batch_size, seq_len, hidden]\n",
    "        \"\"\"\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb603127-e2f7-441e-b123-11765bce6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部のモジュールを一つのクラスでまとめる。\n",
    "class BertLayer(nn.Module):\n",
    "    \"\"\"Transformerブロックになります\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-Attention部分\n",
    "        self.attention = BertAttention(config)\n",
    "\n",
    "        # Self-Attentionの出力を処理する全結合層\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "\n",
    "        # Self-Attentionよる特徴量とBertLayerへの元の入力を足し算する層\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attention=False):\n",
    "        \"\"\"\n",
    "        hidden_states: Embeddingsモジュールもしくは前段のBertLayerからの出力。[batch_size, seq_len, hidden]\n",
    "        attention_mask: Paddingsはソフトマックスに影響を与えないようにする。[batch_size, 1, 1, seq_len]\n",
    "        output_attention: Attentionの重みを出力するか\n",
    "        \"\"\"\n",
    "\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            output_attention=output_attention,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "        # [batch_size, seq_length, hidden_size]\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876bb20c-2254-41a3-a1b8-65bd5cb70a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(1, 12, 128, 128)\n",
    "mask = torch.ones(1, 1, 1, 128)\n",
    "(a + mask)[0][0][0], (a + mask)[0][0][120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd583479-0b61-459a-8f9a-a9438a0ecc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"BertLayerを繰り返すモジュール\"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config[\"num_hidden_layers\"])])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attention=False, output_hidden_states=False):\n",
    "        \"\"\"\n",
    "        hidden_states: Embeddingsモジュールの出力[batch_size, seq_len, hidden]\n",
    "        attention_mask: Paddingsはソフトマックスに影響を与えないようにする。[batch_size, 1, 1, seq_len]\n",
    "        output_all_encoded_layers: 返り値を全TransformerBlockモジュールの出力にするか、\n",
    "        それとも、最終層だけにするかのフラグ。\n",
    "        output_attention: Attentionの重みを出力するか\n",
    "        \"\"\"\n",
    "\n",
    "        # 返り値として使うリスト\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attention else None\n",
    "\n",
    "        \n",
    "        # BertLayerモジュールの処理を繰り返す\n",
    "        for layer_module in self.layer:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "                \n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                output_attention\n",
    "            )\n",
    "            # 返り値はいつもtupleから最初の素をとる\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attention:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        # Noneの変数を排除する\n",
    "        return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, all_hidden_states, all_self_attentions]\n",
    "                if v is not None\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7da89b4-f075-4e8e-97e2-cc248c44b2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([1, 256, 768]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(3, 5, (1,256), dtype=torch.long)\n",
    "embeddings = BertEmbeddings(config)\n",
    "\n",
    "input_tensor = embeddings(a)\n",
    "encoder = BertEncoder(config)\n",
    "\n",
    "len(encoder(input_tensor)), encoder(input_tensor)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26547d65-f9ca-429f-b40d-8474a29dcf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 13,\n",
       " 12,\n",
       " tensor([[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = encoder(input_tensor, output_attention=True, output_hidden_states=True)\n",
    "\n",
    "len(outputs), len(outputs[1]), len(outputs[2]), (outputs[0] == outputs[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c6ccf18-7666-4163-b65b-0bb9e0b64cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([1, 12, 256, 256]), torch.Size([1, 256, 768]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[0]), outputs[2][-1].shape, outputs[1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ecaa8b-f9cb-4188-89fd-b24485509bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    BertEncoderの出力から[CLS]の単語を取ってる、全結合層と活性関数によって\n",
    "    特徴量を変換する。NextSentencePredictionや分類タスクで使う。\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states -> [batch_size, seq_len, hidden_size]\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "\n",
    "        return pooled_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc344ca-9154-427e-ab58-7da4d25de1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]),\n",
       " tensor([ 0.0678,  0.4871, -0.7966, -0.0249, -0.1831,  0.4435,  0.4465, -0.4207,\n",
       "          0.8042, -0.5632, -0.3638, -0.3969, -0.7254, -0.5836, -0.2880, -0.0489,\n",
       "         -0.2480, -0.1460,  0.2313,  0.5132,  0.7189, -0.7814,  0.1698, -0.7711,\n",
       "          0.4066,  0.0424,  0.0098,  0.4318,  0.9087,  0.2121, -0.5832, -0.4336,\n",
       "         -0.2484,  0.1466,  0.1118,  0.5377, -0.1745,  0.0755, -0.1755, -0.0519,\n",
       "          0.0561,  0.2709, -0.5778,  0.3078, -0.5558,  0.4155, -0.5731, -0.0894,\n",
       "         -0.0378, -0.2992], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler = BertPooler(config)\n",
    "\n",
    "pooler(outputs[0]).shape, pooler(outputs[0])[0][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bd9f8cb-4049-42c1-80f4-f866dbc9c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    \"\"\"Transformersのエンコーダーの部分\"\"\"\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "    def forward(self, input_ids,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        output_hidden_states=False,\n",
    "        output_attention=False,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        attention_mask: Paddingsはソフトマックスに影響を与えないようにする。[batch_size, 1, 1, seq_len]\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        output_all_encoded_layers: 返り値を全TransformerBlockモジュールの出力にするか、\n",
    "        それとも、最終層だけにするかのフラグ。\n",
    "        output_attention: Attentionの重みを出力するか\n",
    "        \"\"\"\n",
    "        \n",
    "        # Attentionのマスクと文の1文目、2文目のidが無ければ作成する\n",
    "        # このtorch.ones_likeは入力と同じタイプとディバイス\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # Attention\n",
    "\n",
    "        extended_attention_mask: torch.Tensor = get_extended_attention_mask(attention_mask)\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        # この出力は引数によって変わる。\n",
    "        # output_attention=False, output_hidden_states=False -> ([batch_size, seq_len, hidden_size],)\n",
    "        # output_attention=True, output_hidden_states=False -> ([batch_size, seq_len, hidden_size], 12回([batch_size, num_heads, seq_len, seq_len]))\n",
    "        # output_attention=True, output_hidden_states=True -> ([batch_size, seq_len, hidden_size], \n",
    "        # 13回([batch_size, seq_len, hidden_size]), 12回([batch_size, num_heads, seq_len, seq_len])))\n",
    "        encoder_outputs =  self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            output_attention=output_attention,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        # これはlast_hidden_state（最後の層の出力）[batch_size, seq_len, hidden_size]\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        # [batch_size, hidden_size]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        return (sequence_output, pooled_output) + encoder_outputs[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d780b72-0736-48a7-bf1c-a0b13baadc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_attention_mask(attention_mask: torch.Tensor, dtype: torch.float = torch.float32) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "         Attentionと因果関係のマスクをブロードキャスト可能に作成し、未来のトークンとマスクされたトークンを無視します。\n",
    "        \n",
    "         引数:\n",
    "             attention_mask (`torch.Tensor`):\n",
    "                 注目するトークンに1を示し、無視するトークンに0を示すマスク。\n",
    "        \n",
    "         戻り値:\n",
    "             `torch.Tensor` 拡張されたAttentionマスクで、`attention_mask.dtype` と同じdtypeを持っています。\n",
    "\n",
    "        \"\"\"\n",
    "        extended_attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "        # Attention_maskが1.0は注目させたい位置で、0.0はマスクされた位置です。\n",
    "        # この操作により、注目させたい位置では0.0で、マスクされた位置ではdtypeの最小値となる\n",
    "        # テンソルが作成されます。\n",
    "        # これは実質的には、softmaxの前に生のスコアに追加されているため、\n",
    "        # これを完全に削除するのと同じです。\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=dtype) \n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n",
    "        return extended_attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3c49117-b4dd-424d-a4e0-9ea640dcbc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_layersのテンソルサイズ： torch.Size([2, 5, 768])\n",
      "pooled_outputのテンソルサイズ： torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "# 入力の用意\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# Bertモデルを作成\n",
    "bert = BertModel(config)\n",
    "\n",
    "# 順伝搬させる\n",
    "sequence_output, pooled_output = bert(input_ids, token_type_ids, attention_mask, output_hidden_states=False,\n",
    "        output_attention=False)\n",
    "\n",
    "print(\"encoded_layersのテンソルサイズ：\", sequence_output.shape)\n",
    "print(\"pooled_outputのテンソルサイズ：\", pooled_output.shape)\n",
    "# print(\"attention_probsのテンソルサイズ：\", attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b13f8f4e-ce32-473a-8801-3274270c8e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, torch.Size([2, 5, 768]), torch.Size([2, 768]), 13, 12)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = bert(input_ids, token_type_ids, attention_mask, output_hidden_states=True,\n",
    "        output_attention=True)\n",
    "\n",
    "len(outputs), outputs[0].shape, outputs[1].shape, len(outputs[2]), len(outputs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "918fc7a1-f27c-40b4-8b14-9cba589609a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 768]), torch.Size([2, 12, 5, 5]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2][-1].shape, outputs[3][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3422a257-9bf8-4f46-928a-9a34ada51192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0] == outputs[2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f403a09e-5a5c-492c-a5aa-9ee87d8ec1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.4709, 0.3242, 0.3160],\n",
       "         [0.0000, 0.0000, 0.4529, 0.3695, 0.0000],\n",
       "         [0.0000, 0.0000, 0.3338, 0.4519, 0.0000],\n",
       "         [0.0000, 0.0000, 0.3516, 0.4158, 0.3438],\n",
       "         [0.0000, 0.0000, 0.4611, 0.3391, 0.3110]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.3763, 0.3624, 0.3724],\n",
       "         [0.0000, 0.0000, 0.5063, 0.0000, 0.2820],\n",
       "         [0.0000, 0.0000, 0.3646, 0.3918, 0.3547],\n",
       "         [0.0000, 0.0000, 0.0000, 0.4057, 0.3197],\n",
       "         [0.0000, 0.0000, 0.3702, 0.4167, 0.3243]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.3620, 0.3143],\n",
       "         [0.0000, 0.0000, 0.3126, 0.5222, 0.2763],\n",
       "         [0.0000, 0.0000, 0.3017, 0.4340, 0.3754],\n",
       "         [0.0000, 0.0000, 0.4176, 0.3767, 0.3169],\n",
       "         [0.0000, 0.0000, 0.3645, 0.3814, 0.3652]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.3520, 0.3885, 0.3706],\n",
       "         [0.0000, 0.0000, 0.3105, 0.3840, 0.4166],\n",
       "         [0.0000, 0.0000, 0.3473, 0.3610, 0.4029],\n",
       "         [0.0000, 0.0000, 0.3274, 0.3984, 0.3853],\n",
       "         [0.0000, 0.0000, 0.4070, 0.3254, 0.3788]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.3253, 0.4471, 0.3388],\n",
       "         [0.0000, 0.0000, 0.0000, 0.4827, 0.2941],\n",
       "         [0.0000, 0.0000, 0.3607, 0.3611, 0.3893],\n",
       "         [0.0000, 0.0000, 0.4029, 0.0000, 0.3428],\n",
       "         [0.0000, 0.0000, 0.3631, 0.3887, 0.3593]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.2334, 0.5083, 0.3693],\n",
       "         [0.0000, 0.0000, 0.0000, 0.4396, 0.4102],\n",
       "         [0.0000, 0.0000, 0.2737, 0.4290, 0.4083],\n",
       "         [0.0000, 0.0000, 0.2764, 0.4366, 0.3982],\n",
       "         [0.0000, 0.0000, 0.2910, 0.4542, 0.3659]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.4577, 0.3246, 0.3288],\n",
       "         [0.0000, 0.0000, 0.4330, 0.3669, 0.3112],\n",
       "         [0.0000, 0.0000, 0.3678, 0.3756, 0.3676],\n",
       "         [0.0000, 0.0000, 0.0000, 0.3538, 0.3871],\n",
       "         [0.0000, 0.0000, 0.0000, 0.3631, 0.3614]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.2890, 0.5220, 0.3001],\n",
       "         [0.0000, 0.0000, 0.3586, 0.4452, 0.3073],\n",
       "         [0.0000, 0.0000, 0.3170, 0.5281, 0.2661],\n",
       "         [0.0000, 0.0000, 0.0000, 0.3958, 0.3659],\n",
       "         [0.0000, 0.0000, 0.3581, 0.3766, 0.3764]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.4577, 0.2631, 0.3903],\n",
       "         [0.0000, 0.0000, 0.4085, 0.3582, 0.3445],\n",
       "         [0.0000, 0.0000, 0.4737, 0.2864, 0.3510],\n",
       "         [0.0000, 0.0000, 0.4317, 0.3010, 0.3784],\n",
       "         [0.0000, 0.0000, 0.0000, 0.3228, 0.3665]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.2479, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.2684, 0.0000, 0.2413],\n",
       "         [0.0000, 0.0000, 0.1854, 0.0000, 0.2705],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5969, 0.2645],\n",
       "         [0.0000, 0.0000, 0.2180, 0.5852, 0.3079]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.4188, 0.3331, 0.3592],\n",
       "         [0.0000, 0.0000, 0.3424, 0.3602, 0.4084],\n",
       "         [0.0000, 0.0000, 0.3506, 0.2948, 0.4658],\n",
       "         [0.0000, 0.0000, 0.4305, 0.2561, 0.4245],\n",
       "         [0.0000, 0.0000, 0.3461, 0.3874, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.3725, 0.4064, 0.3323],\n",
       "         [0.0000, 0.0000, 0.0000, 0.4189, 0.3043],\n",
       "         [0.0000, 0.0000, 0.4510, 0.3878, 0.2722],\n",
       "         [0.0000, 0.0000, 0.3485, 0.4692, 0.2935],\n",
       "         [0.0000, 0.0000, 0.4105, 0.3787, 0.3220]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[3][-1][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
